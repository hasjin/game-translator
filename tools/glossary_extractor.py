"""ìš©ì–´ì§‘ ìë™ ì¶”ì¶œ

ê²Œì„ í…ìŠ¤íŠ¸ì—ì„œ ê³ ìœ ëª…ì‚¬, ì „ë¬¸ìš©ì–´ ìë™ ì¶”ì¶œ ë° ê´€ë¦¬
"""
import re
from collections import Counter
from typing import List, Dict, Set
from pathlib import Path
import yaml
import MeCab  # ì¼ë³¸ì–´ í˜•íƒœì†Œ ë¶„ì„ (ì„ íƒì‚¬í•­)


class GlossaryExtractor:
    """ìš©ì–´ì§‘ ìë™ ì¶”ì¶œê¸°"""

    def __init__(self):
        self.proper_nouns = set()
        self.technical_terms = set()
        self.character_names = set()

        # MeCab ì´ˆê¸°í™” (ì¼ë³¸ì–´ ë¶„ì„)
        try:
            self.mecab = MeCab.Tagger()
        except:
            self.mecab = None
            print("âš ï¸ MeCab ì—†ìŒ (ì¼ë³¸ì–´ ë¶„ì„ ì œí•œì )")

    def extract_from_text(self, text: str) -> Dict[str, Set[str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ ì¶”ì¶œ"""
        results = {
            'proper_nouns': set(),
            'katakana_words': set(),
            'kanji_names': set(),
            'technical_terms': set()
        }

        # 1. ì¹´íƒ€ì¹´ë‚˜ ë‹¨ì–´ (ì™¸ë˜ì–´, ê³ ìœ ëª…ì‚¬)
        katakana_pattern = r'[\u30A0-\u30FF]+'
        katakana_words = re.findall(katakana_pattern, text)
        results['katakana_words'].update(katakana_words)

        # 2. í•œì ì´ë¦„ (2-4ê¸€ì)
        kanji_pattern = r'[\u4E00-\u9FFF]{2,4}'
        kanji_words = re.findall(kanji_pattern, text)
        results['kanji_names'].update(kanji_words)

        # 3. MeCab í˜•íƒœì†Œ ë¶„ì„
        if self.mecab:
            parsed = self.mecab.parse(text)
            for line in parsed.split('\n'):
                if '\t' in line:
                    surface, features = line.split('\t')
                    parts = features.split(',')

                    # ê³ ìœ ëª…ì‚¬
                    if len(parts) > 0 and 'åè©' in parts[0]:
                        if 'å›ºæœ‰åè©' in parts[1]:
                            results['proper_nouns'].add(surface)

                        # ì¸ëª…
                        if len(parts) > 1 and 'äººå' in parts[1]:
                            self.character_names.add(surface)

        return results

    def extract_from_files(
        self,
        file_paths: List[str],
        min_frequency: int = 3
    ) -> Dict:
        """ì—¬ëŸ¬ íŒŒì¼ì—ì„œ ìš©ì–´ ì¶”ì¶œ

        Args:
            file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            min_frequency: ìµœì†Œ ì¶œí˜„ íšŸìˆ˜

        Returns:
            ì¶”ì¶œëœ ìš©ì–´ ë”•ì…”ë„ˆë¦¬
        """
        all_terms = Counter()
        all_katakana = Counter()
        all_kanji = Counter()

        for file_path in file_paths:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()

            results = self.extract_from_text(text)

            all_terms.update(results['proper_nouns'])
            all_katakana.update(results['katakana_words'])
            all_kanji.update(results['kanji_names'])

        # ë¹ˆë„ í•„í„°ë§
        glossary = {
            'character_names': [],
            'place_names': [],
            'item_names': [],
            'technical_terms': []
        }

        # ì¸ëª… (ì¹´íƒ€ì¹´ë‚˜ + í•œì)
        for term, freq in all_katakana.most_common():
            if freq >= min_frequency and len(term) >= 2:
                glossary['character_names'].append({
                    'jp': term,
                    'frequency': freq,
                    'type': 'katakana'
                })

        for term, freq in all_kanji.most_common():
            if freq >= min_frequency and 2 <= len(term) <= 4:
                glossary['character_names'].append({
                    'jp': term,
                    'frequency': freq,
                    'type': 'kanji'
                })

        return glossary

    def auto_categorize(self, terms: List[str]) -> Dict[str, List[str]]:
        """ìš©ì–´ ìë™ ë¶„ë¥˜"""
        categories = {
            'character_names': [],
            'place_names': [],
            'item_names': [],
            'skill_names': [],
            'technical_terms': []
        }

        for term in terms:
            # íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜
            if re.search(r'(ã‚¹ã‚­ãƒ«|é­”æ³•|æŠ€|è¡“)', term):
                categories['skill_names'].append(term)
            elif re.search(r'(ç”º|åŸ|å›½|æ‘|ä¸–ç•Œ)', term):
                categories['place_names'].append(term)
            elif re.search(r'(å‰£|ç›¾|é§|è–¬|ã‚¢ã‚¤ãƒ†ãƒ )', term):
                categories['item_names'].append(term)
            elif len(term) <= 6:  # ì§§ìœ¼ë©´ ì¸ëª…ìœ¼ë¡œ ì¶”ì •
                categories['character_names'].append(term)
            else:
                categories['technical_terms'].append(term)

        return categories

    def suggest_translations(
        self,
        term: str,
        existing_glossary: Dict = None
    ) -> List[str]:
        """ë²ˆì—­ ì œì•ˆ

        Args:
            term: ì¼ë³¸ì–´ ìš©ì–´
            existing_glossary: ê¸°ì¡´ ìš©ì–´ì§‘

        Returns:
            ë²ˆì—­ í›„ë³´ ë¦¬ìŠ¤íŠ¸
        """
        suggestions = []

        # 1. ê¸°ì¡´ ìš©ì–´ì§‘ ê²€ìƒ‰
        if existing_glossary and term in existing_glossary:
            suggestions.append(existing_glossary[term])

        # 2. ì¹´íƒ€ì¹´ë‚˜ â†’ í•œê¸€ ìŒì°¨
        if re.match(r'^[\u30A0-\u30FF]+$', term):
            # ì¹´íƒ€ì¹´ë‚˜ ìŒì°¨í‘œ
            katakana_to_hangul = {
                'ã‚¢': 'ì•„', 'ã‚¤': 'ì´', 'ã‚¦': 'ìš°', 'ã‚¨': 'ì—', 'ã‚ª': 'ì˜¤',
                'ã‚«': 'ì¹´', 'ã‚­': 'í‚¤', 'ã‚¯': 'ì¿ ', 'ã‚±': 'ì¼€', 'ã‚³': 'ì½”',
                'ã‚µ': 'ì‚¬', 'ã‚·': 'ì‹œ', 'ã‚¹': 'ìŠ¤', 'ã‚»': 'ì„¸', 'ã‚½': 'ì†Œ',
                'ã‚¿': 'íƒ€', 'ãƒ': 'ì¹˜', 'ãƒ„': 'ì¸ ', 'ãƒ†': 'í…Œ', 'ãƒˆ': 'í† ',
                'ãƒŠ': 'ë‚˜', 'ãƒ‹': 'ë‹ˆ', 'ãƒŒ': 'ëˆ„', 'ãƒ': 'ë„¤', 'ãƒ': 'ë…¸',
                'ãƒ': 'í•˜', 'ãƒ’': 'íˆ', 'ãƒ•': 'í›„', 'ãƒ˜': 'í—¤', 'ãƒ›': 'í˜¸',
                'ãƒ': 'ë§ˆ', 'ãƒŸ': 'ë¯¸', 'ãƒ ': 'ë¬´', 'ãƒ¡': 'ë©”', 'ãƒ¢': 'ëª¨',
                'ãƒ¤': 'ì•¼', 'ãƒ¦': 'ìœ ', 'ãƒ¨': 'ìš”',
                'ãƒ©': 'ë¼', 'ãƒª': 'ë¦¬', 'ãƒ«': 'ë£¨', 'ãƒ¬': 'ë ˆ', 'ãƒ­': 'ë¡œ',
                'ãƒ¯': 'ì™€', 'ãƒ²': 'ì˜¤', 'ãƒ³': 'ã…‡',
                'ã‚¬': 'ê°€', 'ã‚®': 'ê¸°', 'ã‚°': 'êµ¬', 'ã‚²': 'ê²Œ', 'ã‚´': 'ê³ ',
                'ã‚¶': 'ì', 'ã‚¸': 'ì§€', 'ã‚º': 'ì¦ˆ', 'ã‚¼': 'ì œ', 'ã‚¾': 'ì¡°',
                'ãƒ€': 'ë‹¤', 'ãƒ‚': 'ì§€', 'ãƒ…': 'ì¦ˆ', 'ãƒ‡': 'ë°', 'ãƒ‰': 'ë„',
                'ãƒ': 'ë°”', 'ãƒ“': 'ë¹„', 'ãƒ–': 'ë¶€', 'ãƒ™': 'ë² ', 'ãƒœ': 'ë³´',
                'ãƒ‘': 'íŒŒ', 'ãƒ”': 'í”¼', 'ãƒ—': 'í‘¸', 'ãƒš': 'í˜', 'ãƒ': 'í¬',
                'ãƒ£': 'ì•¼', 'ãƒ¥': 'ìœ ', 'ãƒ§': 'ìš”',
                'ãƒƒ': '',  # ì´‰ìŒ
                'ãƒ¼': '',  # ì¥ìŒ
            }

            hangul = ''
            for char in term:
                hangul += katakana_to_hangul.get(char, char)

            suggestions.append(hangul)

        return suggestions

    def export_to_yaml(
        self,
        glossary: Dict,
        output_path: str
    ):
        """YAML ìš©ì–´ì§‘ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(glossary, f, allow_unicode=True, sort_keys=False)

        print(f"âœ… ìš©ì–´ì§‘ ë‚´ë³´ë‚´ê¸°: {output_path}")

    def interactive_review(self, terms: List[str]) -> Dict:
        """ëŒ€í™”í˜• ê²€ìˆ˜"""
        glossary = {}

        print("ğŸ“ ìš©ì–´ì§‘ ê²€ìˆ˜ (Enter=ê±´ë„ˆë›°ê¸°, q=ì¢…ë£Œ)")
        for i, term in enumerate(terms, 1):
            suggestions = self.suggest_translations(term)

            print(f"\n[{i}/{len(terms)}] {term}")
            if suggestions:
                print(f"  ì œì•ˆ: {', '.join(suggestions)}")

            translation = input("  ë²ˆì—­: ").strip()

            if translation.lower() == 'q':
                break
            elif translation:
                glossary[term] = translation

        return glossary


class ContextGlossary:
    """ë¬¸ë§¥ ê¸°ë°˜ ìš©ì–´ì§‘"""

    def __init__(self):
        self.context_map = {}  # {term: [(context, translation), ...]}

    def add_with_context(
        self,
        term: str,
        translation: str,
        context: str
    ):
        """ë¬¸ë§¥ê³¼ í•¨ê»˜ ì¶”ê°€"""
        if term not in self.context_map:
            self.context_map[term] = []

        self.context_map[term].append({
            'context': context,
            'translation': translation
        })

    def find_best_translation(
        self,
        term: str,
        current_context: str
    ) -> Optional[str]:
        """ë¬¸ë§¥ì— ê°€ì¥ ì í•©í•œ ë²ˆì—­ ì°¾ê¸°"""
        if term not in self.context_map:
            return None

        from difflib import SequenceMatcher

        best_match = None
        best_similarity = 0

        for entry in self.context_map[term]:
            similarity = SequenceMatcher(
                None,
                current_context,
                entry['context']
            ).ratio()

            if similarity > best_similarity:
                best_similarity = similarity
                best_match = entry['translation']

        return best_match if best_similarity > 0.6 else None


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    extractor = GlossaryExtractor()

    # íŒŒì¼ì—ì„œ ì¶”ì¶œ
    files = list(Path("translation_work/extracted").glob("**/*.txt"))
    glossary = extractor.extract_from_files(files, min_frequency=5)

    print(f"ì¶”ì¶œëœ ìš©ì–´: {len(glossary['character_names'])}ê°œ")

    # YAML ë‚´ë³´ë‚´ê¸°
    extractor.export_to_yaml(glossary, "auto_glossary.yaml")

    # ëŒ€í™”í˜• ê²€ìˆ˜
    # reviewed = extractor.interactive_review(glossary['character_names'][:10])
