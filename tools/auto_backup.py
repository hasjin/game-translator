"""ìë™ ë°±ì—… ì‹œìŠ¤í…œ

ë²ˆì—­ ì‘ì—… ì¤‘ ìë™ìœ¼ë¡œ ë°±ì—…í•˜ì—¬ ë°ì´í„° ì†ì‹¤ ë°©ì§€
"""
import shutil
from pathlib import Path
from datetime import datetime
import zipfile
import json
from typing import Optional, List
import schedule
import time
import threading


class AutoBackup:
    """ìë™ ë°±ì—… ê´€ë¦¬ì"""

    def __init__(
        self,
        source_dir: str,
        backup_dir: str = "backups",
        max_backups: int = 10
    ):
        self.source_dir = Path(source_dir)
        self.backup_dir = Path(backup_dir)
        self.max_backups = max_backups

        self.backup_dir.mkdir(parents=True, exist_ok=True)

    def create_backup(self, comment: str = "") -> str:
        """ë°±ì—… ìƒì„±

        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{timestamp}.zip"
        backup_path = self.backup_dir / backup_name

        # ZIP ì••ì¶•
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in self.source_dir.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(self.source_dir)
                    zipf.write(file_path, arcname)

        # ë©”íƒ€ë°ì´í„°
        meta = {
            'timestamp': timestamp,
            'comment': comment,
            'file_count': len(list(self.source_dir.rglob('*'))),
            'size_mb': backup_path.stat().st_size / (1024 * 1024)
        }

        meta_path = self.backup_dir / f"backup_{timestamp}.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, indent=2)

        print(f"âœ… ë°±ì—… ìƒì„±: {backup_path} ({meta['size_mb']:.2f}MB)")

        # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
        self.cleanup_old_backups()

        return str(backup_path)

    def restore_backup(self, backup_name: str):
        """ë°±ì—… ë³µì›"""
        backup_path = self.backup_dir / backup_name

        if not backup_path.exists():
            print(f"âŒ ë°±ì—… íŒŒì¼ ì—†ìŒ: {backup_path}")
            return

        # í˜„ì¬ íŒŒì¼ ë°±ì—… (ì•ˆì „)
        safety_backup = self.create_backup(comment="ë³µì› ì „ ìë™ ë°±ì—…")

        # ë³µì›
        with zipfile.ZipFile(backup_path, 'r') as zipf:
            zipf.extractall(self.source_dir)

        print(f"âœ… ë°±ì—… ë³µì› ì™„ë£Œ: {backup_name}")
        print(f"   ì•ˆì „ ë°±ì—…: {safety_backup}")

    def list_backups(self) -> List[Dict]:
        """ë°±ì—… ëª©ë¡"""
        backups = []

        for backup_file in sorted(self.backup_dir.glob("backup_*.zip")):
            timestamp = backup_file.stem.replace("backup_", "")
            meta_file = self.backup_dir / f"backup_{timestamp}.json"

            meta = {}
            if meta_file.exists():
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta = json.load(f)

            backups.append({
                'name': backup_file.name,
                'timestamp': timestamp,
                'size_mb': backup_file.stat().st_size / (1024 * 1024),
                'comment': meta.get('comment', ''),
                'file_count': meta.get('file_count', 0)
            })

        return backups

    def cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ"""
        backups = sorted(self.backup_dir.glob("backup_*.zip"))

        if len(backups) > self.max_backups:
            to_delete = backups[:-self.max_backups]
            for backup in to_delete:
                # ZIP ë° ë©”íƒ€ë°ì´í„° ì‚­ì œ
                backup.unlink()
                meta = backup.parent / f"{backup.stem}.json"
                if meta.exists():
                    meta.unlink()

            print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… {len(to_delete)}ê°œ ì‚­ì œë¨")

    def auto_backup_daemon(self, interval_minutes: int = 30):
        """ë°±ì—… ë°ëª¬ (ë°±ê·¸ë¼ìš´ë“œ ìë™ ë°±ì—…)"""
        def backup_job():
            self.create_backup(comment=f"{interval_minutes}ë¶„ ìë™ ë°±ì—…")

        # ìŠ¤ì¼€ì¤„ ì„¤ì •
        schedule.every(interval_minutes).minutes.do(backup_job)

        # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        def run_schedule():
            while True:
                schedule.run_pending()
                time.sleep(60)

        thread = threading.Thread(target=run_schedule, daemon=True)
        thread.start()

        print(f"ğŸ• ìë™ ë°±ì—… ì‹œì‘: {interval_minutes}ë¶„ë§ˆë‹¤")


class ProgressCheckpoint:
    """ì§„í–‰ ìƒí™© ì²´í¬í¬ì¸íŠ¸"""

    def __init__(self, checkpoint_file: str = "checkpoint.json"):
        self.checkpoint_file = Path(checkpoint_file)
        self.data = self.load()

    def load(self) -> dict:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save(self, data: dict):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        self.data.update(data)
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=2)

    def save_progress(
        self,
        chapter: str,
        file_index: int,
        total_files: int,
        completed_dialogues: int
    ):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        self.save({
            'chapter': chapter,
            'file_index': file_index,
            'total_files': total_files,
            'completed_dialogues': completed_dialogues,
            'timestamp': datetime.now().isoformat(),
            'resume_possible': True
        })

        print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {chapter} [{file_index}/{total_files}]")

    def can_resume(self) -> bool:
        """ì¬ê°œ ê°€ëŠ¥ ì—¬ë¶€"""
        return self.data.get('resume_possible', False)

    def get_resume_point(self) -> dict:
        """ì¬ê°œ ì§€ì  ì •ë³´"""
        return {
            'chapter': self.data.get('chapter'),
            'file_index': self.data.get('file_index', 0),
            'total_files': self.data.get('total_files', 0),
            'completed_dialogues': self.data.get('completed_dialogues', 0)
        }

    def clear(self):
        """ì²´í¬í¬ì¸íŠ¸ í´ë¦¬ì–´"""
        self.data = {}
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()


class VersionControl:
    """ê°„ë‹¨í•œ ë²„ì „ ê´€ë¦¬"""

    def __init__(self, versions_dir: str = "versions"):
        self.versions_dir = Path(versions_dir)
        self.versions_dir.mkdir(parents=True, exist_ok=True)

    def create_version(
        self,
        file_path: str,
        version_name: str = "",
        comment: str = ""
    ):
        """íŒŒì¼ ë²„ì „ ìƒì„±"""
        file_path = Path(file_path)

        if not file_path.exists():
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
            return

        # ë²„ì „ ì´ë¦„
        if not version_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            version_name = f"v_{timestamp}"

        # ë²„ì „ ë””ë ‰í† ë¦¬
        version_dir = self.versions_dir / file_path.stem / version_name
        version_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ ë³µì‚¬
        version_file = version_dir / file_path.name
        shutil.copy2(file_path, version_file)

        # ë©”íƒ€ë°ì´í„°
        meta = {
            'version_name': version_name,
            'original_path': str(file_path),
            'comment': comment,
            'timestamp': datetime.now().isoformat(),
            'size': file_path.stat().st_size
        }

        with open(version_dir / "meta.json", 'w', encoding='utf-8') as f:
            json.dump(meta, f, indent=2)

        print(f"ğŸ“Œ ë²„ì „ ìƒì„±: {version_name} - {comment}")

    def list_versions(self, file_name: str) -> List[Dict]:
        """íŒŒì¼ ë²„ì „ ëª©ë¡"""
        file_versions_dir = self.versions_dir / Path(file_name).stem

        if not file_versions_dir.exists():
            return []

        versions = []
        for version_dir in sorted(file_versions_dir.iterdir()):
            if version_dir.is_dir():
                meta_file = version_dir / "meta.json"
                if meta_file.exists():
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        versions.append(json.load(f))

        return versions

    def restore_version(self, file_name: str, version_name: str):
        """ë²„ì „ ë³µì›"""
        version_dir = self.versions_dir / Path(file_name).stem / version_name
        meta_file = version_dir / "meta.json"

        if not meta_file.exists():
            print(f"âŒ ë²„ì „ ì—†ìŒ: {version_name}")
            return

        with open(meta_file, 'r', encoding='utf-8') as f:
            meta = json.load(f)

        original_path = Path(meta['original_path'])
        version_file = version_dir / original_path.name

        # ë³µì›
        shutil.copy2(version_file, original_path)
        print(f"âœ… ë²„ì „ ë³µì›: {version_name} â†’ {original_path}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ìë™ ë°±ì—…
    backup = AutoBackup(
        source_dir="translation_work/extracted",
        max_backups=10
    )

    # ìˆ˜ë™ ë°±ì—…
    backup.create_backup(comment="ë²ˆì—­ ì‹œì‘ ì „")

    # ë°±ì—… ëª©ë¡
    for b in backup.list_backups():
        print(f"{b['name']}: {b['size_mb']:.2f}MB - {b['comment']}")

    # ìë™ ë°±ì—… ì‹œì‘ (30ë¶„ë§ˆë‹¤)
    # backup.auto_backup_daemon(interval_minutes=30)

    # ì²´í¬í¬ì¸íŠ¸
    checkpoint = ProgressCheckpoint()
    checkpoint.save_progress(
        chapter="Act01_Ch01",
        file_index=10,
        total_files=44,
        completed_dialogues=500
    )

    # ì¬ê°œ
    if checkpoint.can_resume():
        resume_point = checkpoint.get_resume_point()
        print(f"ğŸ“ ì¬ê°œ ê°€ëŠ¥: {resume_point}")
