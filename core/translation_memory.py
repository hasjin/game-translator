"""ë²ˆì—­ ë©”ëª¨ë¦¬ (Translation Memory) ì‹œìŠ¤í…œ

ë™ì¼í•œ ë¬¸ì¥ì€ ì¬ë²ˆì—­í•˜ì§€ ì•Šê³  ì €ì¥ëœ ë²ˆì—­ ì¬ì‚¬ìš©
â†’ ë¹„ìš© ì ˆê° + ì¼ê´€ì„± í–¥ìƒ
"""
import sqlite3
import hashlib
from pathlib import Path
from typing import Optional, List, Dict
from datetime import datetime
import json


class TranslationMemory:
    """ë²ˆì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬"""

    def __init__(self, db_path: str = "translation_memory.db"):
        self.db_path = Path(db_path)
        self.conn = sqlite3.connect(str(self.db_path))
        self.create_tables()

    def create_tables(self):
        """í…Œì´ë¸” ìƒì„±"""
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS translations (
                id TEXT PRIMARY KEY,
                source_text TEXT NOT NULL,
                target_text TEXT NOT NULL,
                source_lang TEXT DEFAULT 'ja',
                target_lang TEXT DEFAULT 'ko',
                engine TEXT,
                context TEXT,
                file_path TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                usage_count INTEGER DEFAULT 1
            )
        ''')

        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS fuzzy_matches (
                source_hash TEXT,
                target_hash TEXT,
                similarity REAL,
                PRIMARY KEY (source_hash, target_hash)
            )
        ''')

        self.conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_source_text
            ON translations(source_text)
        ''')

        self.conn.commit()

    def _generate_id(self, source_text: str, context: str = '') -> str:
        """ê³ ìœ  ID ìƒì„±"""
        raw = f"{source_text}:{context}"
        return hashlib.md5(raw.encode('utf-8')).hexdigest()

    def add(
        self,
        source_text: str,
        target_text: str,
        engine: str = 'unknown',
        context: str = '',
        file_path: str = ''
    ):
        """ë²ˆì—­ ì¶”ê°€"""
        tm_id = self._generate_id(source_text, context)

        self.conn.execute('''
            INSERT OR REPLACE INTO translations
            (id, source_text, target_text, engine, context, file_path, usage_count)
            VALUES (?, ?, ?, ?, ?, ?,
                COALESCE((SELECT usage_count + 1 FROM translations WHERE id = ?), 1))
        ''', (tm_id, source_text, target_text, engine, context, file_path, tm_id))

        self.conn.commit()

    def search(
        self,
        source_text: str,
        context: str = '',
        exact_match: bool = True
    ) -> Optional[Dict]:
        """ë²ˆì—­ ê²€ìƒ‰"""
        if exact_match:
            tm_id = self._generate_id(source_text, context)
            cursor = self.conn.execute('''
                SELECT target_text, engine, usage_count, created_at
                FROM translations
                WHERE id = ?
            ''', (tm_id,))
        else:
            # ìœ ì‚¬ ë§¤ì¹˜
            cursor = self.conn.execute('''
                SELECT target_text, engine, usage_count, created_at
                FROM translations
                WHERE source_text = ?
                ORDER BY usage_count DESC
                LIMIT 1
            ''', (source_text,))

        result = cursor.fetchone()
        if result:
            return {
                'target_text': result[0],
                'engine': result[1],
                'usage_count': result[2],
                'created_at': result[3]
            }
        return None

    def search_fuzzy(self, source_text: str, threshold: float = 0.8) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰"""
        from difflib import SequenceMatcher

        cursor = self.conn.execute('SELECT source_text, target_text FROM translations')
        results = []

        for row in cursor:
            similarity = SequenceMatcher(None, source_text, row[0]).ratio()
            if similarity >= threshold:
                results.append({
                    'source_text': row[0],
                    'target_text': row[1],
                    'similarity': similarity
                })

        return sorted(results, key=lambda x: x['similarity'], reverse=True)

    def batch_add(self, translations: List[Dict]):
        """ë°°ì¹˜ ì¶”ê°€"""
        for trans in translations:
            self.add(
                source_text=trans['source'],
                target_text=trans['target'],
                engine=trans.get('engine', 'unknown'),
                context=trans.get('context', ''),
                file_path=trans.get('file_path', '')
            )

    def export_to_tmx(self, output_path: str):
        """TMX í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (í‘œì¤€ TM í¬ë§·)"""
        cursor = self.conn.execute('''
            SELECT source_text, target_text, created_at
            FROM translations
        ''')

        tmx = '''<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE tmx SYSTEM "tmx14.dtd">
<tmx version="1.4">
<header creationtool="GameTranslator" creationtoolversion="1.0"
        datatype="plaintext" segtype="sentence"
        adminlang="ko" srclang="ja"/>
<body>
'''

        for row in cursor:
            source, target, created = row
            tmx += f'''
<tu creationdate="{created}">
    <tuv xml:lang="ja"><seg>{self._escape_xml(source)}</seg></tuv>
    <tuv xml:lang="ko"><seg>{self._escape_xml(target)}</seg></tuv>
</tu>'''

        tmx += '\n</body>\n</tmx>'

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(tmx)

        print(f"âœ… TMX ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")

    def _escape_xml(self, text: str) -> str:
        """XML ì´ìŠ¤ì¼€ì´í”„"""
        return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')

    def get_statistics(self) -> Dict:
        """í†µê³„ ì •ë³´"""
        cursor = self.conn.execute('SELECT COUNT(*), SUM(usage_count) FROM translations')
        total, total_usage = cursor.fetchone()

        cursor = self.conn.execute('SELECT engine, COUNT(*) FROM translations GROUP BY engine')
        by_engine = dict(cursor.fetchall())

        return {
            'total_entries': total or 0,
            'total_usage': total_usage or 0,
            'by_engine': by_engine,
            'savings': f"${(total_usage - total) * 0.000186:.2f}" if total_usage else "$0.00"
        }

    def cleanup_unused(self, min_usage: int = 1):
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•­ëª© ì •ë¦¬"""
        self.conn.execute('DELETE FROM translations WHERE usage_count < ?', (min_usage,))
        self.conn.commit()

    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.conn.close()


class TMIntegrationMixin:
    """ë²ˆì—­ê¸°ì— TM í†µí•©"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tm = TranslationMemory()

    def translate_with_tm(self, text: str, context: str = '') -> str:
        """TM ìš°ì„  ê²€ìƒ‰ â†’ ì—†ìœ¼ë©´ AI ë²ˆì—­"""
        # 1. TM ê²€ìƒ‰
        cached = self.tm.search(text, context)
        if cached:
            print(f"  ğŸ’¾ TM íˆíŠ¸: {text[:30]}...")
            return cached['target_text']

        # 2. AI ë²ˆì—­
        translation = self.translate(text)

        # 3. TM ì €ì¥
        self.tm.add(text, translation, engine=self.__class__.__name__, context=context)

        return translation

    def translate_batch_with_tm(
        self,
        texts: List[str],
        context: str = ''
    ) -> List[str]:
        """ë°°ì¹˜ ë²ˆì—­ with TM"""
        results = []
        to_translate = []
        indices = []

        # 1. TM ê²€ìƒ‰
        for i, text in enumerate(texts):
            cached = self.tm.search(text, context)
            if cached:
                results.append(cached['target_text'])
                print(f"  ğŸ’¾ TM íˆíŠ¸ [{i+1}/{len(texts)}]")
            else:
                results.append(None)
                to_translate.append(text)
                indices.append(i)

        # 2. ì—†ëŠ” ê²ƒë§Œ AI ë²ˆì—­
        if to_translate:
            print(f"  ğŸ¤– AI ë²ˆì—­: {len(to_translate)}ê°œ")
            translations = self.translate_batch(to_translate)

            # 3. ê²°ê³¼ ë³‘í•© ë° TM ì €ì¥
            for idx, trans in zip(indices, translations):
                results[idx] = trans
                self.tm.add(to_translate[indices.index(idx)], trans,
                           engine=self.__class__.__name__, context=context)

        return results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    tm = TranslationMemory()

    # ë²ˆì—­ ì¶”ê°€
    tm.add("ã“ã‚“ã«ã¡ã¯", "ì•ˆë…•í•˜ì„¸ìš”", engine="claude_haiku")
    tm.add("ã‚ã‚ŠãŒã¨ã†", "ê°ì‚¬í•©ë‹ˆë‹¤", engine="claude_haiku")

    # ê²€ìƒ‰
    result = tm.search("ã“ã‚“ã«ã¡ã¯")
    print(f"ë²ˆì—­: {result['target_text']}, ì‚¬ìš©: {result['usage_count']}íšŒ")

    # í†µê³„
    stats = tm.get_statistics()
    print(f"ì´ í•­ëª©: {stats['total_entries']}")
    print(f"ì´ ì¬ì‚¬ìš©: {stats['total_usage']}")
    print(f"ì ˆê° ë¹„ìš©: {stats['savings']}")

    # TMX ë‚´ë³´ë‚´ê¸°
    tm.export_to_tmx("translation_memory.tmx")
